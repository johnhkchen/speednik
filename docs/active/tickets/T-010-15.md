---
id: T-010-15
story: S-010
title: ppo-agent-and-policy-eval
type: task
status: open
priority: high
phase: research
depends_on: [T-010-14]
---

## Context

Build the `PPOAgent` class that loads a trained CleanRL model checkpoint and conforms to the
`Agent` protocol. This closes the loop: train a policy with CleanRL → load it as an Agent →
run it through scenarios → compare to programmed agents.

See `docs/specs/scenario-testing-system.md` §4.3 for the PPOAgent design.

### PPOAgent class

```python
class PPOAgent:
    """Wraps a trained CleanRL PPO model as an Agent."""

    def __init__(self, model_path: str, device: str = "cpu"):
        self.device = torch.device(device)
        self.model = torch.load(model_path, map_location=self.device)
        self.model.eval()

    def act(self, obs: np.ndarray) -> int:
        with torch.no_grad():
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
            action, _, _, _ = self.model.get_action_and_value(obs_tensor)
            return action.item()

    def reset(self) -> None:
        pass
```

### Model loading

CleanRL's `ppo.py` saves the full `Agent` class (actor + critic networks) via
`torch.save(agent, path)`. The loaded object has `get_action_and_value(obs)` method.
Verify this works with the checkpoint format from T-010-14.

### Agent registry

Register `ppo` in the agent registry so scenario YAML files can reference it:

```yaml
agent: ppo
agent_params:
  model_path: models/ppo_hillside_1M.pt
```

### Evaluation scenario

Write a scenario YAML that evaluates a trained policy:

```yaml
name: hillside_ppo_eval
description: Evaluate trained PPO policy on Hillside Rush
stage: hillside
agent: ppo
agent_params:
  model_path: models/ppo_hillside_latest.pt
max_frames: 3600
success:
  type: goal_reached
failure:
  type: player_dead
metrics:
  - completion_time
  - max_x
  - rings_collected
  - total_reward
  - average_speed
```

### Comparison workflow

The end-to-end workflow:

```bash
# Train
uv run python tools/ppo_speednik.py --total-timesteps 1000000

# Evaluate via scenario
uv run python -m speednik.scenarios.cli scenarios/hillside_ppo_eval.yaml

# Compare to programmed agents
uv run python -m speednik.scenarios.cli --all -o results/ppo_eval.json \
    --compare results/baseline.json
```

### Location

- `speednik/agents/ppo_agent.py` — PPOAgent class
- `speednik/agents/registry.py` — add "ppo" entry (conditional on torch availability)
- `scenarios/hillside_ppo_eval.yaml` — evaluation scenario

## Acceptance Criteria

- [ ] `PPOAgent` conforms to `Agent` protocol
- [ ] `PPOAgent` loads a CleanRL checkpoint and runs inference
- [ ] `PPOAgent` uses `torch.no_grad()` for inference efficiency
- [ ] `ppo` registered in agent registry with conditional torch import
- [ ] If torch not installed, `resolve_agent("ppo")` raises clear error message
- [ ] Evaluation scenario YAML runs end-to-end with a trained model
- [ ] PPOAgent produces deterministic actions (no sampling in eval mode)
- [ ] PPOAgent works on CPU (no GPU required)
- [ ] Scenario comparison works: PPO eval vs programmed agent baseline
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes (PPOAgent tests skip if torch not installed)
