---
id: T-010-13
story: S-010
title: baseline-comparison-and-regression-detection
type: task
status: open
priority: high
phase: done
depends_on: [T-010-12]
---

## Context

Implement the `--compare` flag for the scenario CLI: load a baseline results JSON, compare
it to the current run, and print a human-readable diff showing regressions and improvements.

See `docs/specs/scenario-testing-system.md` §7.3 for the comparison format.

### Comparison output

```
hillside_complete:
  completion_time: 1847 → 1623 (-12.1%)  ✓ faster
  max_x:           3200 → 3200 (0.0%)
  average_speed:   4.2  → 4.8  (+14.3%)  ✓ faster
  rings_collected:  47  →  52  (+10.6%)

hillside_hold_right:
  max_x:           1456 → 1456 (0.0%)
  stuck_at:        1456 → 1456 (0.0%)    ← still stuck

pipeworks_jump:
  completion_time: 2103 → 2340 (+11.3%)  ⚠ slower
  average_speed:   3.8  → 3.2  (-15.8%)  ⚠ regression
```

### Metric directionality

Some metrics are "higher is better" (max_x, rings_collected, average_speed, peak_speed),
some are "lower is better" (completion_time, death_count, stuck_at), and some are neutral.
The comparison formatter needs to know which direction is good:

```python
METRIC_DIRECTION = {
    "completion_time": "lower",
    "max_x": "higher",
    "rings_collected": "higher",
    "death_count": "lower",
    "total_reward": "higher",
    "average_speed": "higher",
    "peak_speed": "higher",
    "time_on_ground": "neutral",
    "stuck_at": "neutral",
}
```

### Regression threshold

Not every change is meaningful. Define a threshold (default 5%) below which changes are
shown but not flagged:

```python
def is_regression(metric, old_val, new_val, threshold=0.05):
    direction = METRIC_DIRECTION.get(metric, "neutral")
    if direction == "neutral":
        return False
    delta_pct = (new_val - old_val) / abs(old_val) if old_val else 0
    if direction == "higher":
        return delta_pct < -threshold
    return delta_pct > threshold
```

### Pass/fail status change

The most important comparison: did any scenario flip from PASS to FAIL (or vice versa)?
Print this prominently at the top:

```
⚠ STATUS CHANGES:
  hillside_hold_right: FAIL → PASS  (fixed!)
  skybridge_spindash:  PASS → FAIL  (REGRESSION)
```

### Exit code

`--compare` should influence the exit code:
- Exit 0: no regressions
- Exit 1: any scenario flipped PASS→FAIL
- Exit 2: significant metric regressions (above threshold) but no status flips

### Create initial baseline

After all scenarios pass, generate a baseline:

```bash
uv run python -m speednik.scenarios.cli --all -o results/baseline.json
```

Commit this to the repo so future runs can `--compare results/baseline.json`.

### Location

- `speednik/scenarios/compare.py` — comparison logic
- `speednik/scenarios/cli.py` — wire compare into CLI
- `results/` — directory for baseline and run results (gitignore run results, commit
  baseline)

## Acceptance Criteria

- [ ] `--compare baseline.json` loads baseline and prints per-scenario metric diff
- [ ] Percentage change shown for each metric
- [ ] Regressions flagged with warning symbol based on metric directionality
- [ ] Status changes (PASS↔FAIL) printed prominently
- [ ] Regression threshold (default 5%) filters noise
- [ ] Exit code 0 for no regressions, 1 for status flips, 2 for metric regressions
- [ ] Handles missing scenarios in baseline (new scenario, prints "NEW")
- [ ] Handles missing scenarios in current run (removed scenario, prints "MISSING")
- [ ] `results/` directory created with `.gitkeep`
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
