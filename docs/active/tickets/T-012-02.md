---
id: T-012-02
story: S-012
title: hillside-behavior-audit
type: task
status: open
priority: high
phase: done
depends_on: [T-012-01]
---

## Context

Run all 6 player archetypes through Hillside Rush and document every finding. Hillside is
the easiest stage — the tutorial level. Every archetype except Chaos should be able to make
significant progress. A Walker should be able to reach the goal on a well-designed tutorial
stage.

### Your role

You are a QA auditor. Your job is to run the archetypes, observe what happens, and document
findings. **Do not adjust expectations to match current behavior.** If the Walker can't get
past x=601, that's a bug to file — not a threshold to lower.

### Expected behaviors for Hillside

| Archetype    | min_x   | max_deaths | goal? | rationale                                |
|--------------|---------|------------|-------|------------------------------------------|
| Walker       | 4700    | 0          | yes   | Tutorial level, flat terrain, should work |
| Jumper       | 4700    | 0          | yes   | Jumping only helps on this stage          |
| Speed Demon  | 4700    | 0          | yes   | Spindash clears everything                |
| Cautious     | 2400    | 0          | no    | Slow, may not finish in frame budget      |
| Wall Hugger  | 2400    | 0          | no    | Wall recovery tests, slower progress      |
| Chaos        | 1200    | 2          | no    | Random input, just needs forward drift    |

These expectations are **aspirational** — they describe what a good tutorial level should
deliver. When the game falls short, that's a finding.

### Process

For each archetype:
1. Call `run_audit("hillside", make_<archetype>(), expectation)`
2. Examine findings: stuck positions, invariant violations, deaths
3. For each finding:
   - Is it a real bug (collision wall, clipping, physics glitch)?
   - Or is it expected behavior (Chaos walks off a cliff)?
4. Write the test with the correct expectation
5. If the test fails due to a real bug, mark it `@pytest.mark.xfail(reason="BUG: ...")`
   and create a bug ticket in `docs/active/tickets/`

### Bug ticket format

When you find a bug, create a ticket:
```yaml
---
id: T-012-02-BUG-01
story: S-012
title: hillside-wall-at-x601
type: bug
status: open
priority: high
phase: ready
depends_on: []
---

## Finding

Walker strategy gets stuck at x=601 in Hillside Rush.

## Evidence

- Frame 6: angle jumps to 64 (quadrant 1, wall) at x=601.4
- ground_speed drops to -0.01 and oscillates
- Player never advances past x=602

## Expected

Player should walk smoothly from x=600 through the ramp approach at x=700.

## Reproduction

```python
run_audit("hillside", make_walker(), HILLSIDE_WALKER_EXPECTATION)
```

## Probable cause

Tile at tx=37 or tx=38 has angle=64 (wall angle) where it should be a gentle slope.
```

### Location

`tests/test_audit_hillside.py` — all hillside behavior tests.
Bug tickets in `docs/active/tickets/T-012-02-BUG-*.md` as discovered.

## Acceptance Criteria

- [ ] 6 archetype tests for hillside (Walker, Jumper, Speed Demon, Cautious, Wall Hugger, Chaos)
- [ ] Each test uses the expectation table above (not weakened)
- [ ] Findings documented in assertion messages via `format_findings()`
- [ ] Real bugs marked `@pytest.mark.xfail` with bug ticket reference
- [ ] Bug tickets created in `docs/active/tickets/` for each real finding
- [ ] No expectations adjusted to match current broken behavior
- [ ] `uv run pytest tests/test_audit_hillside.py -v` runs (xfails are fine, unexpected failures are not)
