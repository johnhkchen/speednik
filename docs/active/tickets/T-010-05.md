---
id: T-010-05
story: S-010
title: programmed-agents
type: task
status: open
priority: high
phase: research
depends_on: [T-010-04]
---

## Context

Port the S-008 harness strategies to observation-based agents that conform to the `Agent`
protocol. Each agent reads only the observation vector — no direct `Player` access.

See `docs/specs/scenario-testing-system.md` §4.2 for the reference implementations.

### Agents

**HoldRightAgent** — Always returns `ACTION_RIGHT`. The simplest possible agent and the
baseline for every scenario.

**JumpRunnerAgent** — Runs right, jumps when the forward terrain ray is close (obstacle
detected) or upon landing. Uses `obs[4]` (on_ground) and later `obs[18]` (forward ray
distance) when raycasts are available. Initial version without raycasts can jump periodically
or when on_ground transitions from False→True.

**SpindashAgent** — State machine: CROUCH → CHARGE (N frames) → RELEASE → RUN. Re-dashes
when `ground_speed` (obs[5]) drops below a threshold. See spec §4.2 for the full
implementation.

**ScriptedAgent** — Takes a timeline of `(start_frame, end_frame, action)` tuples. Returns
the action for the active window. Tracks its own frame counter. For precise frame-by-frame
control in scenario definitions.

**IdleAgent** — Always returns `ACTION_NOOP`. For ground adhesion tests.

### Compatibility with harness strategies

The S-008 harness strategies (`tests/harness.py`) use `(frame, Player) → InputState`. The
new agents use `(obs) → int`. These are different interfaces for different contexts:

- Harness strategies: direct player access, for focused physics tests
- Protocol agents: observation-only, for scenarios and RL

Both should produce similar (not identical) behavior for the same logical strategy. The
observation-based agents may differ slightly because they use normalized obs values instead
of raw physics state.

### Agent registry

A simple dict mapping agent names (strings) to classes, for scenario YAML resolution:

```python
AGENT_REGISTRY = {
    "idle": IdleAgent,
    "hold_right": HoldRightAgent,
    "jump_runner": JumpRunnerAgent,
    "spindash": SpindashAgent,
    "scripted": ScriptedAgent,
}

def resolve_agent(name: str, params: dict | None = None) -> Agent:
    cls = AGENT_REGISTRY[name]
    return cls(**(params or {}))
```

### Location

- `speednik/agents/hold_right.py`
- `speednik/agents/jump_runner.py`
- `speednik/agents/spindash.py`
- `speednik/agents/scripted.py`
- `speednik/agents/idle.py`
- `speednik/agents/registry.py` — agent name → class mapping

## Acceptance Criteria

- [ ] HoldRightAgent conforms to Agent protocol, always returns ACTION_RIGHT
- [ ] JumpRunnerAgent conforms to Agent protocol, runs right + jumps at obstacles/landings
- [ ] SpindashAgent conforms to Agent protocol, implements charge/release/run/re-dash cycle
- [ ] ScriptedAgent conforms to Agent protocol, plays back a frame-indexed timeline
- [ ] IdleAgent conforms to Agent protocol, always returns ACTION_NOOP
- [ ] All agents pass `isinstance(agent, Agent)` check
- [ ] `resolve_agent("hold_right")` returns a working HoldRightAgent
- [ ] `resolve_agent("spindash", {"charge_frames": 5})` passes kwargs
- [ ] Smoke test: HoldRightAgent on hillside sim for 300 frames, x increases
- [ ] Smoke test: SpindashAgent on hillside sim for 300 frames, reaches higher x than
  HoldRightAgent
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
