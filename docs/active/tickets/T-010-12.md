---
id: T-010-12
story: S-010
title: trajectory-serialization-and-metrics
type: task
status: open
priority: high
phase: done
depends_on: [T-010-11]
---

## Context

Implement trajectory serialization to JSON and the metrics computation pipeline. Trajectories
are the raw data; metrics are the summary statistics extracted from them.

See `docs/specs/scenario-testing-system.md` §7.1–7.2 for metrics and format.

### Per-scenario metrics

| Metric | Type | Description |
|--------|------|-------------|
| `completion_time` | `int\|null` | Frames to success (null if failed) |
| `max_x` | `float` | Furthest rightward position |
| `rings_collected` | `int` | Total rings picked up |
| `death_count` | `int` | Times player died |
| `total_reward` | `float` | Sum of per-frame rewards |
| `average_speed` | `float` | Mean \|x_vel\| across all frames |
| `peak_speed` | `float` | Maximum \|x_vel\| observed |
| `time_on_ground` | `float` | Fraction of frames on_ground=True |
| `stuck_at` | `float\|null` | X where player stalled (or null) |

### compute_metrics function

```python
def compute_metrics(
    requested: list[str],
    trajectory: list[FrameRecord],
    sim: SimState,
) -> dict[str, Any]:
    """Compute requested metrics from trajectory and final sim state."""
```

Only compute metrics listed in the scenario's `metrics` field. Unknown metric names raise
a clear error. The `velocity_profile` metric returns the full list of x_vel values (large —
only included with `--trajectory`).

### JSON serialization

```json
{
  "name": "hillside_complete",
  "success": true,
  "reason": "goal_reached",
  "frames_elapsed": 1847,
  "wall_time_ms": 42.3,
  "metrics": {
    "completion_time": 1847,
    "max_x": 3200.5,
    "rings_collected": 47,
    "average_speed": 4.2,
    "peak_speed": 12.1
  },
  "trajectory": [...]
}
```

Trajectories are large (3600 frames x ~12 fields). By default omit them from JSON output.
The `--trajectory` CLI flag includes them. The `save_results` function handles this toggle.

### Trajectory efficiency

Consider using a compact format for trajectories when `--trajectory` is used:
- Column-oriented: `{"x": [1.0, 1.5, ...], "y": [...]}` instead of row-oriented
- Or omit redundant fields (events are usually empty)
- Or use msgpack/cbor if JSON is too slow (defer unless it's a problem)

For now, row-oriented JSON is fine. Optimize later if needed.

### Location

- `speednik/scenarios/metrics.py` — compute_metrics, metric definitions
- `speednik/scenarios/output.py` — extend save_results with serialization logic

## Acceptance Criteria

- [ ] All 9 metrics from the table computed correctly
- [ ] `compute_metrics` only computes requested metrics
- [ ] Unknown metric name raises ValueError with clear message
- [ ] `velocity_profile` metric returns full x_vel list
- [ ] JSON output includes name, success, reason, frames, wall_time, metrics
- [ ] Trajectory omitted by default, included with `--trajectory`
- [ ] JSON output is valid and parseable
- [ ] Round-trip test: save → load → compare fields
- [ ] `stuck_at` uses sliding window variance (reuses harness concept)
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
