---
id: T-010-07
story: S-010
title: reward-signal-and-observation-space
type: task
status: open
priority: high
phase: done
depends_on: [T-010-06]
---

## Context

Implement the reward function and finalize the observation space for the Gymnasium
environment. The reward signal is the most important design decision for RL training quality.

See `docs/specs/scenario-testing-system.md` §3.2 (observations) and §3.3 (reward).

### Reward signal

```python
def _compute_reward(self, events) -> float:
    reward = 0.0
    sim = self.sim
    p = sim.player.physics

    # Primary: delta max(x) — rightward progress into new territory
    new_max = max(sim.max_x_reached, p.x)
    progress_delta = (new_max - sim.max_x_reached) / sim.level_width
    reward += progress_delta * 10.0

    # Speed bonus: reward maintaining high horizontal speed
    reward += abs(p.x_vel) / MAX_X_SPEED * 0.01

    # Goal completion: large bonus scaled by remaining time
    if sim.goal_reached:
        time_bonus = max(0.0, 1.0 - self._step_count / self.max_steps)
        reward += 10.0 + 5.0 * time_bonus

    # Death penalty
    if sim.player_dead:
        reward -= 5.0

    # Ring collection: small bonus per ring
    for e in events:
        if isinstance(e, RingCollectedEvent):
            reward += 0.1

    # Time penalty: small per-frame cost to discourage stalling
    reward -= 0.001

    return reward
```

**Design rationale** (from spec): `delta_max(x)` is proven for Sonic-style games (OpenAI
Retro Contest 2018). It only rewards reaching new rightward positions, so backtracking
through loops or over obstacles has zero penalty. The speed bonus is small enough to not
dominate but encourages momentum maintenance.

### Observation space finalization

Wire the 12-dim simplified observation from T-010-04 into the env's `_get_obs()`. Ensure
the `observation_space` Box shape matches `OBS_DIM`. The full 26-dim vector (with raycasts)
comes later in T-010-17 — the env should be designed so swapping `OBS_DIM` from 12 to 26
is a one-line change.

### Reward scaling tests

Write tests that verify reward behavior:

- Standing still for 100 frames → negative total reward (time penalty dominates)
- Moving right for 100 frames → positive total reward (progress dominates)
- Reaching goal → large positive spike
- Dying → large negative spike
- Collecting rings → small positive per ring

### Location

`speednik/env.py` — extend from T-010-06.

## Acceptance Criteria

- [ ] `_compute_reward` implements delta_max(x) progress reward
- [ ] Speed bonus rewards horizontal velocity
- [ ] Goal completion gives large bonus scaled by remaining time
- [ ] Death gives negative reward
- [ ] Ring collection gives small positive reward
- [ ] Time penalty discourages stalling
- [ ] `_get_obs()` returns properly shaped numpy array matching `observation_space`
- [ ] Observation values are normalized (roughly [-1, 1] range)
- [ ] Reward test: idle agent gets negative total over 100 frames
- [ ] Reward test: hold_right agent gets positive total over 100 frames
- [ ] Reward test: goal reached produces reward spike > 10.0
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
