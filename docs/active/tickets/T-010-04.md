---
id: T-010-04
story: S-010
title: agent-protocol-and-observation-extraction
type: task
status: open
priority: high
phase: done
depends_on: [T-010-02]
---

## Context

Define the `Agent` protocol and the observation extraction function. This is Layer 3 of the
architecture — the interface that all agents (programmed and RL) conform to.

See `docs/specs/scenario-testing-system.md` §4.1 for the protocol and §3.2 for observations.

### Agent protocol

```python
@runtime_checkable
class Agent(Protocol):
    def act(self, obs: np.ndarray) -> int:
        """Given an observation vector, return a discrete action index."""
        ...

    def reset(self) -> None:
        """Called at episode start. Reset internal state if any."""
        ...
```

Using `Protocol` (not ABC) so agents don't need to inherit — duck typing with type checking.

### Action space

8 discrete actions mapping to `InputState` combinations:

```python
ACTION_NOOP       = 0
ACTION_LEFT       = 1
ACTION_RIGHT      = 2
ACTION_JUMP       = 3
ACTION_LEFT_JUMP  = 4
ACTION_RIGHT_JUMP = 5
ACTION_DOWN       = 6
ACTION_DOWN_JUMP  = 7

ACTION_MAP = { ... }  # int → InputState
```

See spec §3.1 for the full mapping. Note the `jump_pressed` vs `jump_held` handling —
`jump_pressed` should only be True on the first frame of a jump action. The caller (env or
runner) tracks `_prev_jump_held` to manage this.

### Observation extraction

A function that reads a `SimState` and returns a flat numpy vector. The initial version
omits terrain raycasts (those come in T-010-16/17) and uses a simplified observation:

```python
OBS_DIM_SIMPLE = 12  # Without raycasts

def extract_observation(sim: SimState) -> np.ndarray:
    p = sim.player.physics
    obs = np.zeros(OBS_DIM_SIMPLE, dtype=np.float32)
    # Player kinematics (6)
    obs[0] = p.x / sim.level_width
    obs[1] = p.y / sim.level_height
    obs[2] = p.x_vel / MAX_X_SPEED
    obs[3] = p.y_vel / MAX_X_SPEED
    obs[4] = float(p.on_ground)
    obs[5] = p.ground_speed / MAX_X_SPEED
    # Player state (3)
    obs[6] = float(p.is_rolling)
    obs[7] = float(p.facing_right)
    obs[8] = p.angle / 255.0
    # Progress (3)
    obs[9]  = sim.max_x_reached / sim.level_width
    obs[10] = (sim.goal_x - p.x) / sim.level_width
    obs[11] = float(sim.frame) / 3600.0  # assume 60s default
    return obs
```

This is enough for programmed agents and initial RL experiments. T-010-16/17 add the 14
raycast values to reach the full 26-dim vector.

### action_to_input helper

```python
def action_to_input(action: int, prev_jump_held: bool) -> tuple[InputState, bool]:
    """Convert action int to InputState, managing jump_pressed edge detection.
    Returns (input_state, new_prev_jump_held)."""
```

### Location

- `speednik/agents/__init__.py` — package init
- `speednik/agents/base.py` — Agent protocol
- `speednik/agents/actions.py` — action constants, ACTION_MAP, action_to_input
- `speednik/observation.py` — extract_observation function

## Acceptance Criteria

- [ ] `Agent` protocol defined with `act(obs) → int` and `reset()`
- [ ] `@runtime_checkable` so `isinstance` works
- [ ] 8 discrete actions defined with `ACTION_MAP` to `InputState`
- [ ] `action_to_input` handles `jump_pressed` edge detection across frames
- [ ] `extract_observation` returns 12-dim numpy float32 vector
- [ ] Observation values normalized to roughly [-1, 1] range
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
