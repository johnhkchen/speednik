---
id: T-012-01
story: S-012
title: archetype-library-and-expectation-framework
type: task
status: open
priority: high
phase: done
depends_on: [T-011-02]
---

## Context

Build the player archetype strategies and the test framework that treats failures as
findings rather than broken tests.

### Your role

You are a QA auditor, not a developer making tests pass. When you write an assertion, you
are stating what a player **should** experience. If the game doesn't deliver that experience,
your job is to document the finding — not adjust the assertion.

### Player archetypes

Implement 6 strategy functions that model real player behaviors. Each takes
`(frame: int, sim: SimState) -> InputState`.

**Walker** — `make_walker()`
Hold right every frame. The most basic human input. If a walker can't traverse flat ground,
something is fundamentally broken.

**Jumper** — `make_jumper()`
Hold right + press jump whenever grounded. Models the player who mashes jump. Should clear
gaps, reach platforms, and not get stuck on ceilings.

**Speed Demon** — `make_speed_demon()`
Spindash, release, hold right, re-spindash when slow. Models the power player. Tests
high-speed collision at loop entry, spring chains, and ramp transitions.

**Cautious** — `make_cautious()`
Walk right slowly (tap right, release, repeat). Stop on slopes. Occasionally walk left.
Models the explorer. Tests slope adhesion when speed is low, idle physics on angled surfaces,
and whether backtracking causes stuck states.

**Wall Hugger** — `make_wall_hugger()`
Hold right until velocity drops to near-zero (wall detected), then jump. Repeat. Models the
player stuck against a wall. Tests wall collision recovery and whether jumping at a wall
leads to clipping or stuck states.

**Chaos** — `make_chaos(seed: int)`
Deterministic pseudo-random inputs. Every 5-15 frames, pick a random combination of
right/left/jump/down. Models unpredictable input. Uses a seeded RNG so results are
reproducible.

### Expectation framework

**`BehaviorExpectation` dataclass:**
```python
@dataclass
class BehaviorExpectation:
    name: str
    stage: str
    archetype: str
    min_x_progress: float       # Player should reach at least this X
    max_deaths: int             # Deaths should not exceed this
    require_goal: bool          # Should the player reach the goal?
    max_frames: int             # Frame budget
    invariant_errors_ok: int    # Expected invariant errors (0 for clean levels)
```

**`AuditFinding` dataclass:**
```python
@dataclass
class AuditFinding:
    expectation: str           # What was expected
    actual: str                # What happened
    frame: int                 # When
    x: float                   # Where
    y: float
    severity: str              # "bug" | "warning"
    details: str               # Human-readable explanation
```

**`run_audit()` function:**
```python
def run_audit(
    stage: str,
    archetype_fn,
    expectation: BehaviorExpectation,
) -> tuple[list[AuditFinding], ProbeResult]:
```

Runs the archetype on the stage, collects trajectory + events, runs the invariant checker,
and compares against the expectation. Returns findings (not assertions — the caller decides
whether to assert or xfail).

### xfail workflow

Each per-stage audit test (T-012-02 through T-012-04) will have test methods like:

```python
def test_walker_traverses_hillside(self):
    findings, result = run_audit("hillside", make_walker(), HILLSIDE_WALKER)
    bugs = [f for f in findings if f.severity == "bug"]
    assert len(bugs) == 0, format_findings(bugs)
```

When the test finds a real bug:
1. **Do NOT adjust the expectation** to make it pass
2. Add `@pytest.mark.xfail(reason="BUG: description, see T-XXX-XX")`
3. Create a bug ticket in `docs/active/tickets/` describing the finding
4. Include the frame number, coordinates, and what happened vs. what should happen
5. Continue to the next test

### format_findings helper

A function that formats findings into a readable string for assertion messages:
```
FINDINGS (2 bugs):
  [frame 6, x=601.4] Walker stuck: ground_speed=-0.01 after hitting wall.
    Expected: forward progress past x=700 (ramp region)
    Actual: player oscillates at x=601 with angle=64 (wall)
  [frame 142, x=1832.0] Walker stuck: 30 frames without X progress.
    Expected: continuous forward movement
    Actual: soft-lock at x=1832
```

### Location

`speednik/qa.py` — archetypes, expectations, `run_audit`, findings.
Not in tests/ — this is a reusable library for all audit tickets.

`tests/test_qa_framework.py` — unit tests for the framework itself (synthetic grids,
verify findings are generated correctly for known bad terrain).

## Acceptance Criteria

- [ ] 6 archetype strategy functions implemented and tested on synthetic grids
- [ ] `BehaviorExpectation` and `AuditFinding` dataclasses defined
- [ ] `run_audit()` runs an archetype, collects trajectory, runs invariant checker, returns findings
- [ ] `format_findings()` produces readable output for assertion messages
- [ ] Chaos archetype uses seeded RNG for deterministic reproducibility
- [ ] Framework unit tests verify findings are generated for known bad terrain
- [ ] Framework unit tests verify clean terrain produces 0 findings
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/test_qa_framework.py -x` passes
