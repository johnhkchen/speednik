---
id: T-010-06
story: S-010
title: speednikenv-core
type: task
status: open
priority: high
phase: done
depends_on: [T-010-03, T-010-05]
---

## Context

Build the core `SpeednikEnv` Gymnasium environment class with `reset()`, `step()`, and
action mapping. This is Layer 5 — the bridge between the simulation and RL training.

See `docs/specs/scenario-testing-system.md` §3.1 for the class structure.

### SpeednikEnv

```python
class SpeednikEnv(gym.Env):
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 60}

    def __init__(self, stage: str = "hillside", render_mode=None, max_steps=3600):
        super().__init__()
        self.stage_name = stage
        self.render_mode = render_mode
        self.max_steps = max_steps

        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(OBS_DIM,),
            dtype=np.float32,
        )
        self.action_space = spaces.Discrete(NUM_ACTIONS)

        self.sim: SimState | None = None
        self._step_count = 0
        self._prev_jump_held = False

    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)
        self.sim = create_sim(self.stage_name)
        self._step_count = 0
        self._prev_jump_held = False
        return self._get_obs(), self._get_info()

    def step(self, action: int):
        inp = self._action_to_input(action)
        events = sim_step(self.sim, inp)
        self._step_count += 1

        obs = self._get_obs()
        reward = self._compute_reward(events)
        terminated = self.sim.goal_reached or self.sim.player_dead
        truncated = self._step_count >= self.max_steps
        info = self._get_info()

        return obs, reward, terminated, truncated, info
```

### Action-to-input mapping

Uses `ACTION_MAP` from T-010-04. The critical subtlety: `jump_pressed` must only be True
on the first frame of a jump action. Track `_prev_jump_held` across steps. See spec §3.1
`_action_to_input()`.

### _get_info

Return a dict with debugging-useful state:

```python
def _get_info(self):
    return {
        "frame": self.sim.frame,
        "x": self.sim.player.physics.x,
        "y": self.sim.player.physics.y,
        "max_x": self.sim.max_x_reached,
        "rings": self.sim.rings_collected,
        "deaths": self.sim.deaths,
        "goal_reached": self.sim.goal_reached,
    }
```

### Observation and reward

This ticket wires up `_get_obs()` and `_compute_reward()` as stubs that call the functions
from T-010-04 (observation) and T-010-07 (reward). Use the simplified 12-dim observation
initially. Reward can be a placeholder (e.g., return 0.0) until T-010-07 implements it.

### Location

`speednik/env.py` — new file.

## Acceptance Criteria

- [ ] `SpeednikEnv` subclasses `gym.Env`
- [ ] `__init__` accepts `stage`, `render_mode`, `max_steps`
- [ ] `reset()` creates a fresh simulation, returns (obs, info)
- [ ] `step(action)` advances simulation, returns (obs, reward, terminated, truncated, info)
- [ ] `_action_to_input` correctly maps 8 actions to InputState
- [ ] `jump_pressed` edge detection works across frames (True only on first frame)
- [ ] `terminated` is True when goal reached or player dead
- [ ] `truncated` is True when step count exceeds max_steps
- [ ] `_get_info` returns frame, position, max_x, rings, deaths, goal_reached
- [ ] Multiple reset/step cycles work without errors
- [ ] No Pyxel imports
- [ ] `uv run pytest tests/ -x` passes
