---
id: S-012
title: automated-qa-player-behavior-audits
type: story
status: open
priority: high
tickets: [T-012-01, T-012-02, T-012-03, T-012-04, T-012-05, T-012-06, T-012-07, T-012-08]
---

## Goal

Replace human QA grinding with automated player-behavior audits that surface bugs in stage
geometry, collision, physics, and game objects. The agent's job is to write tests that
describe what a player **should** experience — and when reality doesn't match, that's a
finding to document, not a test to weaken.

## Background

S-011 built test infrastructure (sim_step, invariant checker, walkthrough smoke tests).
But those tests were framed as "make pytest green" — which incentivizes the agent to adjust
coordinates and loosen tolerances until everything passes. That's backwards. The game has
known bugs (collision walls, broken geometry, unreachable sections). Tests should **find**
those bugs, not route around them.

A human QA tester doesn't adjust their test plan when they find a bug. They file it. This
story creates the same workflow for automated testing:

1. Define what players should be able to do (the expectation)
2. Run simulations with player archetypes
3. When reality doesn't match the expectation, that's a finding
4. Mark the test `xfail` with a bug ticket reference
5. The bug ticket tracks the fix; the test stays as-is

The output of this story is not a green test suite — it's a **bug inventory** with test
evidence. A fully green suite means the game works. xfails mean known bugs are tracked.
Unexpected failures mean regressions.

## Key principle

**Tests describe intended behavior. The game conforms to the tests, not the other way
around.** When an agent writes `assert max_x > 700` and the player gets stuck at x=601,
the correct response is:

```python
@pytest.mark.xfail(reason="BUG: hillside wall collision at x=601, see T-XXX-XX")
def test_walker_reaches_ramp():
    ...
    assert max_x > 700  # Player SHOULD reach the ramp
```

Not:

```python
def test_walker_reaches_ramp():
    ...
    assert max_x > 590  # Adjusted to current behavior ← WRONG
```

## Player archetypes

Real players exhibit distinct behavior patterns. Each archetype is a strategy function that
models one pattern:

| Archetype       | Behavior                        | What it tests                        |
|-----------------|---------------------------------|--------------------------------------|
| **Walker**      | Hold right only                 | Basic traversability of terrain      |
| **Jumper**      | Hold right + spam jump          | Gap clearing, ceiling collision      |
| **Speed Demon** | Spindash + hold right           | High-speed collision, loops, springs |
| **Cautious**    | Walk, stop on slopes, backtrack | Slope adhesion, idle physics         |
| **Wall Hugger** | Run into walls, jump at walls   | Wall collision recovery, unsticking  |
| **Chaos**       | Random inputs every N frames    | Chaos monkey for edge cases          |

## Composable mechanic probes

The archetype audits test full stages end-to-end. But game levels are built from composable
elements — loops, ramps, springs, gaps, rails — and each element has preconditions for
correct behavior. A loop requires entry speed ≥ X and a specific angle transition. A spring
requires the player to be grounded and within the hitbox. A gap requires jump timing.

If we can't enter a loop on a synthetic grid under ideal conditions, the loop is broken as
a building block — no amount of stage-level testing will make it work. These mechanic probes
test the **elements** in isolation:

- "What speed do you need to enter and exit a loop cleanly?"
- "Does a spring launch reach the expected height?"
- "Can a 3-tile gap be cleared with a running jump?"
- "Does a ramp transition preserve speed or slam the player into a wall?"

When a mechanic probe fails, it tells you the building block is broken — independent of any
particular stage. This is cheaper and more diagnostic than discovering the same bug through
a full-stage archetype audit.

## Architecture

```
T-012-01  Archetype library + expectation framework + xfail workflow
   │
   ├──→ T-012-02  Hillside full behavior audit
   ├──→ T-012-03  Pipeworks full behavior audit     4 concurrent
   ├──→ T-012-04  Skybridge full behavior audit
   ├──→ T-012-05  Cross-stage behavioral invariants
   │
   └──→ T-012-06  Composable mechanic probes
```

## DAG

```
Gate: [T-011-02]

T-012-01  (archetypes + framework)
   ├──→ T-012-02  (hillside audit)      ┐
   ├──→ T-012-03  (pipeworks audit)      │
   ├──→ T-012-04  (skybridge audit)      │ 5 concurrent
   ├──→ T-012-05  (cross-stage)          │
   └──→ T-012-06  (mechanic probes)      ┘
```

## Pipeline bugfix

The hillside and pipeworks audits surfaced the same root cause: `svg2stage.py` produces
angle=64 (wall) tiles where gentle slopes should be. Hillside has one at tile (37,38),
pipeworks has an entire column at tx=32. The pipeline's `_compute_segment_angle` function
computes the correct angle from SVG geometry, but short or near-vertical line segments in
the SVG produce wall angles that get assigned to surface tiles that should be walkable.

The pipeline already has `_check_accidental_walls` validation, but it only warns about runs
of steep tiles longer than `MAX_STEEP_RUN` — single isolated wall tiles (like hillside's)
slip through. The fix is both to correct the angle computation for edge cases and to
strengthen the validator to auto-fix isolated wall angles by interpolating from neighbors.

T-012-07 fixes the pipeline, regenerates stages, and verifies the audit xfails flip to
passes.

```
T-012-02 + T-012-03  (audit findings)
   └──→ T-012-07  (svg2stage angle fix + regenerate + re-audit)
```

Critical path: 3 tickets deep (01 → 02/03 → 07).
